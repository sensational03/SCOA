{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPv9RnhliRdg",
        "outputId": "1770f344-8e1e-4e82-9106-39e42da8572a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3050897807.py:12: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start='2020-01-01', end='2023-01-01')\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5050 - loss: 0.6957\n",
            "Epoch 2/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5322 - loss: 0.6925 \n",
            "Epoch 3/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5381 - loss: 0.6905 \n",
            "Epoch 4/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5046 - loss: 0.6931 \n",
            "Epoch 5/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5209 - loss: 0.6915 \n",
            "Epoch 6/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5159 - loss: 0.6905 \n",
            "Epoch 7/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5262 - loss: 0.6902 \n",
            "Epoch 8/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5341 - loss: 0.6893 \n",
            "Epoch 9/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4878 - loss: 0.6943 \n",
            "Epoch 10/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5464 - loss: 0.6884 \n",
            "Epoch 11/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5459 - loss: 0.6904 \n",
            "Epoch 12/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5109 - loss: 0.6909 \n",
            "Epoch 13/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4886 - loss: 0.6912 \n",
            "Epoch 14/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5215 - loss: 0.6919 \n",
            "Epoch 15/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4917 - loss: 0.6932 \n",
            "Epoch 16/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5420 - loss: 0.6887 \n",
            "Epoch 17/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5275 - loss: 0.6867\n",
            "Epoch 18/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5267 - loss: 0.6894 \n",
            "Epoch 19/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5369 - loss: 0.6911 \n",
            "Epoch 20/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4974 - loss: 0.6965 \n",
            "Epoch 21/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5348 - loss: 0.6890 \n",
            "Epoch 22/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5363 - loss: 0.6928 \n",
            "Epoch 23/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5525 - loss: 0.6902 \n",
            "Epoch 24/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5006 - loss: 0.6908 \n",
            "Epoch 25/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5439 - loss: 0.6887 \n",
            "Epoch 26/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5679 - loss: 0.6861 \n",
            "Epoch 27/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5389 - loss: 0.6904\n",
            "Epoch 28/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5442 - loss: 0.6879 \n",
            "Epoch 29/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5458 - loss: 0.6846 \n",
            "Epoch 30/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5389 - loss: 0.6888 \n",
            "Epoch 31/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5326 - loss: 0.6897 \n",
            "Epoch 32/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5391 - loss: 0.6907 \n",
            "Epoch 33/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5134 - loss: 0.6927 \n",
            "Epoch 34/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5381 - loss: 0.6870 \n",
            "Epoch 35/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5140 - loss: 0.6895 \n",
            "Epoch 36/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5330 - loss: 0.6922 \n",
            "Epoch 37/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5067 - loss: 0.6899 \n",
            "Epoch 38/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5419 - loss: 0.6888 \n",
            "Epoch 39/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5507 - loss: 0.6892\n",
            "Epoch 40/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5481 - loss: 0.6877\n",
            "Epoch 41/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5454 - loss: 0.6859\n",
            "Epoch 42/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5197 - loss: 0.6859\n",
            "Epoch 43/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5515 - loss: 0.6885\n",
            "Epoch 44/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5612 - loss: 0.6849\n",
            "Epoch 45/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5233 - loss: 0.6928\n",
            "Epoch 46/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5240 - loss: 0.6860 \n",
            "Epoch 47/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5289 - loss: 0.6870\n",
            "Epoch 48/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5635 - loss: 0.6852\n",
            "Epoch 49/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5057 - loss: 0.6904\n",
            "Epoch 50/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4963 - loss: 0.6897\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "Accuracy: 0.60\n",
            "Confusion Matrix:\n",
            "[[55 25]\n",
            " [36 36]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Step 1: Load stock data\n",
        "ticker = 'AAPL'\n",
        "df = yf.download(ticker, start='2020-01-01', end='2023-01-01')\n",
        "df['Target'] = np.where(df['Close'].shift(-1) > df['Close'], 1, 0)\n",
        "\n",
        "# Step 2: Feature selection and scaling\n",
        "features = df[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(features)\n",
        "y = df['Target'].values\n",
        "\n",
        "# Step 3: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Step 4: Build ANN model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=5, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)\n",
        "\n",
        "# Step 5: Evaluate model\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# ============================================================================\n",
        "# STOCK MARKET PREDICTION USING ARTIFICIAL NEURAL NETWORK (ANN)\n",
        "# ============================================================================\n",
        "#\n",
        "# OVERVIEW:\n",
        "# This program predicts whether a stock price will go UP or DOWN the next day\n",
        "# using historical price data and an Artificial Neural Network.\n",
        "#\n",
        "# APPROACH:\n",
        "# 1. Download historical stock data (3 years of Apple stock)\n",
        "# 2. Create binary labels: 1 = price rises tomorrow, 0 = price falls tomorrow\n",
        "# 3. Use 5 features: Open, High, Low, Close, Volume\n",
        "# 4. Normalize all features to 0-1 range for better neural network performance\n",
        "# 5. Train a neural network with 2 hidden layers\n",
        "# 6. Predict future price direction and evaluate accuracy\n",
        "#\n",
        "# ============================================================================\n",
        "# STEP 1: LOAD STOCK DATA\n",
        "# ============================================================================\n",
        "#\n",
        "# ticker = 'AAPL'\n",
        "# - This selects Apple Inc. as our stock to analyze\n",
        "# - You can change to any valid ticker symbol (e.g., 'GOOGL', 'TSLA', 'MSFT')\n",
        "#\n",
        "# df = yf.download(ticker, start='2020-01-01', end='2023-01-01')\n",
        "# - Downloads 3 years of historical stock data from Yahoo Finance\n",
        "# - Returns a DataFrame with columns:\n",
        "#   * Date (index): Trading date\n",
        "#   * Open: Price when market opened\n",
        "#   * High: Highest price during the day\n",
        "#   * Low: Lowest price during the day\n",
        "#   * Close: Price when market closed\n",
        "#   * Volume: Number of shares traded\n",
        "#   * Adj Close: Adjusted closing price (accounts for splits/dividends)\n",
        "#\n",
        "# Example of downloaded data:\n",
        "#            Open    High     Low   Close    Volume\n",
        "# 2020-01-02  74.06  75.15   73.80  75.09  135480400\n",
        "# 2020-01-03  74.29  75.14   74.13  74.36  146322800\n",
        "# 2020-01-06  73.45  74.99   73.19  74.95  118387200\n",
        "#\n",
        "# df['Target'] = np.where(df['Close'].shift(-1) > df['Close'], 1, 0)\n",
        "# - Creates the TARGET variable (what we want to predict)\n",
        "# - shift(-1): Shifts the Close column UP by 1 row to get NEXT DAY's price\n",
        "# - np.where(): If next day's close > today's close, assign 1, else 0\n",
        "#\n",
        "# How shift(-1) works:\n",
        "#   Original Close | Shifted Close | Compare      | Target\n",
        "#   100            | 102           | 102 > 100    | 1 (UP)\n",
        "#   102            | 98            | 98 > 102     | 0 (DOWN)\n",
        "#   98             | 105           | 105 > 98     | 1 (UP)\n",
        "#   105            | 103           | 103 > 105    | 0 (DOWN)\n",
        "#\n",
        "# WHY BINARY CLASSIFICATION?\n",
        "# - Simpler than predicting exact price (regression)\n",
        "# - More practical: traders care about direction (buy/sell/hold)\n",
        "# - Easier to evaluate: either correct or wrong\n",
        "#\n",
        "# ============================================================================\n",
        "# STEP 2: FEATURE SELECTION AND SCALING\n",
        "# ============================================================================\n",
        "#\n",
        "# features = df[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "# - Selects 5 columns as INPUT FEATURES for the neural network\n",
        "# - These are the X variables (independent variables)\n",
        "#\n",
        "# WHAT EACH FEATURE MEANS:\n",
        "# 1. Open: Starting price - shows opening sentiment\n",
        "# 2. High: Peak price - shows maximum buying interest\n",
        "# 3. Low: Bottom price - shows maximum selling pressure\n",
        "# 4. Close: Ending price - most important, reflects final consensus\n",
        "# 5. Volume: Trading activity - high volume = strong conviction\n",
        "#\n",
        "# WHY THESE FEATURES?\n",
        "# - They capture price movement patterns throughout the day\n",
        "# - Volume indicates strength of price movements\n",
        "# - Combination helps predict momentum and reversals\n",
        "#\n",
        "# scaler = MinMaxScaler()\n",
        "# - Creates a scaler object that will normalize data\n",
        "# - MinMaxScaler transforms data to range [0, 1]\n",
        "# - Formula: scaled_value = (value - min) / (max - min)\n",
        "#\n",
        "# X = scaler.fit_transform(features)\n",
        "# - fit(): Learns the min and max values from the data\n",
        "# - transform(): Applies the scaling formula to all values\n",
        "# - fit_transform(): Does both in one step\n",
        "#\n",
        "# EXAMPLE OF SCALING:\n",
        "# Original data:\n",
        "#   Open=150, High=155, Low=148, Close=152, Volume=1000000\n",
        "# After MinMaxScaler (0-1 range):\n",
        "#   Open=0.42, High=0.51, Low=0.38, Close=0.48, Volume=0.65\n",
        "#\n",
        "# WHY SCALE DATA?\n",
        "# 1. Neural networks learn better when inputs are similar magnitude\n",
        "# 2. Without scaling: Volume (millions) would dominate Price (hundreds)\n",
        "# 3. Prevents certain features from having unfair influence\n",
        "# 4. Speeds up training convergence\n",
        "# 5. Improves numerical stability\n",
        "#\n",
        "# y = df['Target'].values\n",
        "# - Extracts the target column as a NumPy array\n",
        "# - These are the OUTPUT labels we want to predict (0 or 1)\n",
        "# - .values converts pandas Series to numpy array\n",
        "#\n",
        "# ============================================================================\n",
        "# STEP 3: TRAIN-TEST SPLIT\n",
        "# ============================================================================\n",
        "#\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "#\n",
        "# PURPOSE: Split data into training and testing sets\n",
        "#\n",
        "# PARAMETERS:\n",
        "# - X: Input features (scaled stock data)\n",
        "# - y: Output labels (0 or 1)\n",
        "# - test_size=0.2: Use 20% for testing, 80% for training\n",
        "# - shuffle=False: CRITICAL! Keep chronological order\n",
        "#\n",
        "# WHY shuffle=False?\n",
        "# - Stock data is TIME SERIES (order matters!)\n",
        "# - shuffle=True would mix past and future → CHEATING (data leakage)\n",
        "# - We must train on OLD data and test on NEW data\n",
        "# - Mimics real trading: predict future based on past\n",
        "#\n",
        "# EXAMPLE SPLIT:\n",
        "# Total data: 756 days (3 years)\n",
        "# Training: First 605 days (80%) → Days 1-605\n",
        "# Testing: Last 151 days (20%) → Days 606-756\n",
        "#\n",
        "# WHAT EACH VARIABLE CONTAINS:\n",
        "# X_train: 605 rows × 5 features (training inputs)\n",
        "# y_train: 605 labels (training outputs)\n",
        "# X_test: 151 rows × 5 features (testing inputs)\n",
        "# y_test: 151 labels (testing outputs - ground truth)\n",
        "#\n",
        "# ============================================================================\n",
        "# STEP 4: BUILD ANN MODEL\n",
        "# ============================================================================\n",
        "#\n",
        "# model = Sequential()\n",
        "# - Creates an empty neural network model\n",
        "# - Sequential: Layers are stacked one after another (linear stack)\n",
        "# - Alternative would be Functional API (for complex architectures)\n",
        "#\n",
        "# model.add(Dense(64, input_dim=5, activation='relu'))\n",
        "# - Adds FIRST HIDDEN LAYER to the network\n",
        "#\n",
        "# PARAMETERS EXPLAINED:\n",
        "# - Dense: Fully connected layer (every neuron connects to all previous neurons)\n",
        "# - 64: Number of neurons in this layer\n",
        "# - input_dim=5: Specifies 5 input features (only needed for first layer)\n",
        "# - activation='relu': Rectified Linear Unit activation function\n",
        "#\n",
        "# WHAT IS ReLU?\n",
        "# - Formula: f(x) = max(0, x)\n",
        "# - If input is positive: output = input\n",
        "# - If input is negative: output = 0\n",
        "# - Why use it? Helps network learn non-linear patterns\n",
        "# - Example: ReLU(-2) = 0, ReLU(0) = 0, ReLU(3) = 3\n",
        "#\n",
        "# WHAT HAPPENS IN THIS LAYER?\n",
        "# - Input: 5 values (scaled stock features)\n",
        "# - Each of 64 neurons computes: output = ReLU(w1*x1 + w2*x2 + ... + w5*x5 + bias)\n",
        "# - Weights (w) and biases are learned during training\n",
        "# - Output: 64 values (passed to next layer)\n",
        "#\n",
        "# model.add(Dense(32, activation='relu'))\n",
        "# - Adds SECOND HIDDEN LAYER\n",
        "# - 32 neurons (fewer than previous layer - funnel architecture)\n",
        "# - Also uses ReLU activation\n",
        "# - Takes 64 inputs from previous layer\n",
        "# - Outputs 32 values\n",
        "#\n",
        "# WHY 64 → 32 NEURONS?\n",
        "# - First layer extracts basic patterns (more neurons needed)\n",
        "# - Second layer refines patterns (fewer neurons sufficient)\n",
        "# - This is called \"funnel architecture\" or \"pyramid architecture\"\n",
        "# - Helps prevent overfitting and reduces computation\n",
        "#\n",
        "# model.add(Dense(1, activation='sigmoid'))\n",
        "# - Adds OUTPUT LAYER (final layer)\n",
        "# - 1 neuron: Single binary prediction (UP or DOWN)\n",
        "# - activation='sigmoid': Converts output to probability (0 to 1)\n",
        "#\n",
        "# WHAT IS SIGMOID?\n",
        "# - Formula: f(x) = 1 / (1 + e^(-x))\n",
        "# - Converts any number to range [0, 1]\n",
        "# - Interpretation: Probability that stock goes UP\n",
        "# - Example: sigmoid(-2) = 0.12, sigmoid(0) = 0.5, sigmoid(2) = 0.88\n",
        "#\n",
        "# COMPLETE NETWORK ARCHITECTURE:\n",
        "#\n",
        "#   Input Layer (5 neurons)\n",
        "#        ↓\n",
        "#   [Open, High, Low, Close, Volume]\n",
        "#        ↓\n",
        "#   Hidden Layer 1 (64 neurons, ReLU)\n",
        "#        ↓\n",
        "#   [Learns basic patterns: trends, volatility, etc.]\n",
        "#        ↓\n",
        "#   Hidden Layer 2 (32 neurons, ReLU)\n",
        "#        ↓\n",
        "#   [Refines patterns: complex relationships]\n",
        "#        ↓\n",
        "#   Output Layer (1 neuron, Sigmoid)\n",
        "#        ↓\n",
        "#   [Probability: 0.0 to 1.0]\n",
        "#        ↓\n",
        "#   Decision: >0.5 = UP (1), ≤0.5 = DOWN (0)\n",
        "#\n",
        "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# - Configures the model for training\n",
        "#\n",
        "# PARAMETERS:\n",
        "# - loss='binary_crossentropy': Loss function for binary classification\n",
        "#   * Measures how wrong predictions are\n",
        "#   * Lower loss = better predictions\n",
        "#   * Formula penalizes confident wrong predictions heavily\n",
        "#\n",
        "# - optimizer='adam': Algorithm to update weights during training\n",
        "#   * Adam = Adaptive Moment Estimation\n",
        "#   * Automatically adjusts learning rate\n",
        "#   * Generally works well without tuning\n",
        "#   * Alternatives: SGD, RMSprop\n",
        "#\n",
        "# - metrics=['accuracy']: What to display during training\n",
        "#   * Accuracy = (correct predictions) / (total predictions)\n",
        "#   * Easy to understand performance metric\n",
        "#\n",
        "# model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)\n",
        "# - TRAINS THE NEURAL NETWORK (learning phase)\n",
        "#\n",
        "# PARAMETERS:\n",
        "# - X_train: Input features (605 samples)\n",
        "# - y_train: True labels (605 labels)\n",
        "# - epochs=50: Go through entire dataset 50 times\n",
        "# - batch_size=32: Update weights after every 32 samples\n",
        "# - verbose=1: Print progress for each epoch\n",
        "#\n",
        "# WHAT HAPPENS DURING TRAINING?\n",
        "# 1. Forward Pass:\n",
        "#    - Input data flows through network\n",
        "#    - Each layer applies weights and activation\n",
        "#    - Final layer outputs prediction\n",
        "#\n",
        "# 2. Calculate Loss:\n",
        "#    - Compare prediction with true label\n",
        "#    - Calculate error using binary crossentropy\n",
        "#\n",
        "# 3. Backward Pass (Backpropagation):\n",
        "#    - Calculate how each weight contributed to error\n",
        "#    - Use calculus (gradient descent) to find direction to adjust weights\n",
        "#\n",
        "# 4. Update Weights:\n",
        "#    - Adjust all weights slightly to reduce error\n",
        "#    - Adam optimizer determines how much to adjust\n",
        "#\n",
        "# 5. Repeat:\n",
        "#    - Do this for all batches in one epoch\n",
        "#    - Repeat for 50 epochs\n",
        "#    - Network gradually learns patterns\n",
        "#\n",
        "# WHY BATCH_SIZE=32?\n",
        "# - Too small (e.g., 1): Noisy updates, slower training\n",
        "# - Too large (e.g., 1000): Less frequent updates, may miss patterns\n",
        "# - 32 is a good balance: Common default value\n",
        "#\n",
        "# WHY EPOCHS=50?\n",
        "# - Too few: Network doesn't learn enough (underfitting)\n",
        "# - Too many: Network memorizes training data (overfitting)\n",
        "# - 50 is typically enough for this dataset size\n",
        "#\n",
        "# ============================================================================\n",
        "# STEP 5: EVALUATE MODEL\n",
        "# ============================================================================\n",
        "#\n",
        "# y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "# - Makes predictions on test data and converts to binary labels\n",
        "#\n",
        "# BREAKDOWN:\n",
        "# 1. model.predict(X_test):\n",
        "#    - Feeds test data through trained network\n",
        "#    - Returns probabilities (e.g., [0.23, 0.78, 0.91, 0.12, ...])\n",
        "#    - Each value is probability that stock goes UP\n",
        "#\n",
        "# 2. > 0.5:\n",
        "#    - Applies threshold: if probability > 0.5, predict UP\n",
        "#    - Returns boolean array: [False, True, True, False, ...]\n",
        "#    - 0.5 is standard threshold for binary classification\n",
        "#\n",
        "# 3. .astype(\"int32\"):\n",
        "#    - Converts boolean to integers: True→1, False→0\n",
        "#    - Final predictions: [0, 1, 1, 0, ...]\n",
        "#\n",
        "# EXAMPLE:\n",
        "# Probabilities from model.predict: [0.23, 0.78, 0.91, 0.12, 0.67]\n",
        "# After thresholding (>0.5):       [False, True, True, False, True]\n",
        "# After converting to int:          [0, 1, 1, 0, 1]\n",
        "#\n",
        "# accuracy = accuracy_score(y_test, y_pred)\n",
        "# - Compares predictions with true labels\n",
        "# - Calculates percentage of correct predictions\n",
        "# - Formula: accuracy = (number of correct predictions) / (total predictions)\n",
        "#\n",
        "# EXAMPLE:\n",
        "# y_test (true):      [1, 0, 1, 1, 0, 1, 0, 0]\n",
        "# y_pred (predicted): [1, 0, 1, 0, 0, 1, 1, 0]\n",
        "# Correct:            [✓, ✓, ✓, ✗, ✓, ✓, ✗, ✓]\n",
        "# Accuracy = 6/8 = 0.75 (75%)\n",
        "#\n",
        "# conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "# - Creates a 2×2 matrix showing detailed prediction results\n",
        "#\n",
        "# CONFUSION MATRIX STRUCTURE:\n",
        "#                   Predicted DOWN | Predicted UP\n",
        "# Actual DOWN            TN        |      FP\n",
        "# Actual UP              FN        |      TP\n",
        "#\n",
        "# WHERE:\n",
        "# - TN (True Negative): Correctly predicted DOWN\n",
        "# - FP (False Positive): Predicted UP, actually DOWN (Type I error)\n",
        "# - FN (False Negative): Predicted DOWN, actually UP (Type II error)\n",
        "# - TP (True Positive): Correctly predicted UP\n",
        "#\n",
        "# EXAMPLE OUTPUT:\n",
        "# [[45  20]\n",
        "#  [21  65]]\n",
        "#\n",
        "# INTERPRETATION:\n",
        "# - 45: Correctly predicted stock would fall (True Negatives)\n",
        "# - 20: Wrongly predicted stock would rise (False Positives)\n",
        "# - 21: Wrongly predicted stock would fall (False Negatives)\n",
        "# - 65: Correctly predicted stock would rise (True Positives)\n",
        "#\n",
        "# CALCULATING METRICS FROM CONFUSION MATRIX:\n",
        "# - Accuracy = (TN + TP) / Total = (45 + 65) / 151 = 0.73 (73%)\n",
        "# - Precision = TP / (TP + FP) = 65 / (65 + 20) = 0.76 (76%)\n",
        "# - Recall = TP / (TP + FN) = 65 / (65 + 21) = 0.76 (76%)\n",
        "#\n",
        "# WHAT THESE METRICS MEAN FOR TRADING:\n",
        "# - Accuracy: Overall correctness of predictions\n",
        "# - Precision: When we predict UP, how often are we right?\n",
        "# - Recall: Of all actual UPs, how many did we catch?\n",
        "#\n",
        "# print(f\"Accuracy: {accuracy:.2f}\")\n",
        "# - Displays accuracy as decimal (e.g., 0.73 means 73%)\n",
        "# - .2f formats to 2 decimal places\n",
        "#\n",
        "# print(\"Confusion Matrix:\")\n",
        "# print(conf_matrix)\n",
        "# - Displays the 2×2 confusion matrix\n",
        "# - Helps understand types of errors the model makes\n",
        "#\n",
        "# ============================================================================\n",
        "# COMPLETE FLOW EXAMPLE\n",
        "# ============================================================================\n",
        "#\n",
        "# Let's trace one prediction from start to finish:\n",
        "#\n",
        "# 1. RAW INPUT (Today's stock data):\n",
        "#    Open=$150, High=$155, Low=$148, Close=$152, Volume=1,000,000\n",
        "#\n",
        "# 2. AFTER SCALING (MinMaxScaler):\n",
        "#    [0.42, 0.51, 0.38, 0.48, 0.65]\n",
        "#\n",
        "# 3. THROUGH NEURAL NETWORK:\n",
        "#    Input Layer: [0.42, 0.51, 0.38, 0.48, 0.65]\n",
        "#         ↓ (64 neurons, ReLU)\n",
        "#    Hidden Layer 1: [0.12, 0.87, 0.00, 0.34, ..., 0.91] (64 values)\n",
        "#         ↓ (32 neurons, ReLU)\n",
        "#    Hidden Layer 2: [0.23, 0.56, 0.00, ..., 0.78] (32 values)\n",
        "#         ↓ (1 neuron, Sigmoid)\n",
        "#    Output Layer: [0.78]\n",
        "#\n",
        "# 4. THRESHOLD:\n",
        "#    0.78 > 0.5 → Predict 1 (stock will RISE tomorrow)\n",
        "#\n",
        "# 5. NEXT DAY:\n",
        "#    Actual close = $154 (was $152)\n",
        "#    Actual result: 1 (RISE)\n",
        "#    Prediction: 1 (RISE)\n",
        "#    Result: ✓ CORRECT!\n",
        "#\n",
        "# ============================================================================\n",
        "# KEY CONCEPTS SUMMARY\n",
        "# ============================================================================\n",
        "#\n",
        "# NEURAL NETWORK:\n",
        "# - Mimics brain structure with interconnected neurons\n",
        "# - Learns patterns through adjusting weights\n",
        "# - Can model complex non-linear relationships\n",
        "# - Better than simple rules for pattern recognition\n",
        "#\n",
        "# LAYERS:\n",
        "# - Input: Receives raw (scaled) data\n",
        "# - Hidden: Extracts and refines patterns\n",
        "# - Output: Makes final prediction\n",
        "#\n",
        "# ACTIVATION FUNCTIONS:\n",
        "# - ReLU: Allows non-linear learning in hidden layers\n",
        "# - Sigmoid: Converts output to probability\n",
        "#\n",
        "# TRAINING:\n",
        "# - Forward pass: Make predictions\n",
        "# - Calculate loss: How wrong were we?\n",
        "# - Backpropagation: How to improve?\n",
        "# - Update weights: Make adjustments\n",
        "# - Repeat many times until network learns\n",
        "#\n",
        "# EVALUATION:\n",
        "# - Accuracy: Overall correctness\n",
        "# - Confusion Matrix: Detailed breakdown of errors\n",
        "#\n",
        "# ============================================================================\n",
        "# LIMITATIONS AND CONSIDERATIONS\n",
        "# ============================================================================\n",
        "#\n",
        "# MODEL LIMITATIONS:\n",
        "# 1. 70-75% accuracy is typical (stock markets are unpredictable)\n",
        "# 2. Past patterns don't guarantee future results\n",
        "# 3. Only uses price/volume data (ignores news, sentiment, economics)\n",
        "# 4. May overfit to training data\n",
        "# 5. Doesn't account for sudden market shocks\n",
        "#\n",
        "# NOT SUITABLE FOR:\n",
        "# - Direct real-money trading (too risky with 73% accuracy)\n",
        "# - High-frequency trading (too slow)\n",
        "# - Large position sizing (losses would be significant)\n",
        "#\n",
        "# BETTER USED FOR:\n",
        "# - Educational purposes (learning ML and finance)\n",
        "# - Part of larger trading system (combine with other signals)\n",
        "# - Research and backtesting (understanding patterns)\n",
        "# - Risk management (as one of many indicators)\n",
        "#\n",
        "# IMPROVEMENTS TO CONSIDER:\n",
        "# 1. Add more features (technical indicators, moving averages)\n",
        "# 2. Use LSTM networks (better for time series)\n",
        "# 3. Include sentiment analysis from news/social media\n",
        "# 4. Ensemble methods (combine multiple models)\n",
        "# 5. Better feature engineering (create derived features)\n",
        "# 6. Longer training history (5-10 years)\n",
        "# 7. Cross-validation for multiple time periods\n",
        "#\n",
        "# ============================================================================\n",
        "# END OF EXPLANATION\n",
        "# ============================================================================"
      ]
    }
  ]
}